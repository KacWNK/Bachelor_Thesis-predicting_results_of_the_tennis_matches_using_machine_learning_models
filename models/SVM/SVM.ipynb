{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kstest, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.resetwarnings()\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"default\", ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(\"../../preparation_before_models/data/matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=matches.drop(columns=['target'])\n",
    "y=matches['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standaryzujemy zmienne, które mają rozkład normalny, do identyfikacji użyjemy testu kolmogorova-smirnova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with normal ds: ['diff_age', 'diff_elo', 'diff_surface_elo', 'diff_blended_elo']\n"
     ]
    }
   ],
   "source": [
    "def identify_normal_columns(X):\n",
    "    normal_columns = []\n",
    "    for col in X.columns:\n",
    "        column_values = X[col]\n",
    "        standarized_column = (column_values - np.mean(column_values)) / np.std(column_values)\n",
    "        stat, p = kstest(standarized_column, 'norm')\n",
    "        if p > 0.05:  # Hipoteza zerowa: zmienna ma rozkład normalny\n",
    "            normal_columns.append(col)\n",
    "    return normal_columns\n",
    "\n",
    "normal_columns = identify_normal_columns(X_train)\n",
    "print(f\"Columns with normal ds: {normal_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standaryzacja tylko dla zmiennych o rozkładzie normalnym\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "\n",
    "X_train_scaled[normal_columns] = scaler.fit_transform(X_train[normal_columns])\n",
    "X_test_scaled[normal_columns] = scaler.transform(X_test[normal_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funkcja do wyboru top% cech według shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_shap(X, y, percent):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    # Obliczenie średniej absolutnej ważności cech\n",
    "    feature_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "    # Wybór cech na podstawie procentowego progu\n",
    "    importance_threshold = np.percentile(feature_importance, 100 - percent)\n",
    "    selected_features = np.where(feature_importance >= importance_threshold)[0]\n",
    "\n",
    "    print(f\"Selected {len(selected_features)} features out of {X.shape[1]} with top {percent}%.\")\n",
    "    return X.iloc[:, selected_features], selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funkcja celu dla optuny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_logloss(trial, X_train, y_train):\n",
    "    kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 1e3, log=True),\n",
    "        \"kernel\": kernel\n",
    "    }\n",
    "    if kernel in [\"rbf\", \"poly\"]:\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-4, 1e0, log=True)\n",
    "    if kernel == \"poly\":\n",
    "        params[\"degree\"] = trial.suggest_int(\"degree\", 2, 5)\n",
    "\n",
    "    model = SVC(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
    "\n",
    "    return -scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie w pętli accuracy dla różnego % cech i optymalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing top 50% features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kacper\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "c:\\Users\\Kacper\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "[I 2024-12-19 16:22:22,421] A new study created in memory with name: no-name-bdcbe084-562e-4bce-b95d-35067ca93f68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 21 features out of 41 with top 50%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\AppData\\Local\\Temp\\ipykernel_6764\\91906397.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"C\": trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
      "C:\\Users\\Kacper\\AppData\\Local\\Temp\\ipykernel_6764\\91906397.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  params[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-4, 1e0)\n",
      "c:\\Users\\Kacper\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "percentages = [ 50, 65, 75, 85, 100]  # Procent cech do wybrania\n",
    "best_logloss = float(\"inf\")\n",
    "best_features = None\n",
    "best_params = None\n",
    "best_num_features = 0\n",
    "\n",
    "for percent in percentages:\n",
    "    print(f\"Testing top {percent}% features...\")\n",
    "    \n",
    "    # Jeśli procent wynosi 100%, nie przeprowadzamy selekcji\n",
    "    if percent == 100:\n",
    "        X_train_selected, selected_features = X_train_scaled, list(range(X_train_scaled.shape[1]))\n",
    "    else:\n",
    "        X_train_selected, selected_features = feature_selection_with_shap(X_train_scaled, y_train, percent)\n",
    "    \n",
    "    X_test_selected = X_test_scaled.iloc[:, selected_features]\n",
    "\n",
    "    # Optuna optymalizacja dla aktualnego podzbioru cech\n",
    "    def wrapped_objective(trial):\n",
    "        return objective_svm_logloss(trial, X_train_selected, y_train)\n",
    "\n",
    "    study_svm_logloss = optuna.create_study(direction=\"minimize\")\n",
    "    study_svm_logloss.optimize(wrapped_objective, n_trials=100, timeout=3600)\n",
    "\n",
    "    if study_svm_logloss.best_value < best_logloss:\n",
    "        best_logloss = study_svm_logloss.best_value\n",
    "        best_features = selected_features\n",
    "        best_params = study_svm_logloss.best_params\n",
    "        best_num_features = len(selected_features)\n",
    "\n",
    "print(f\"Best logloss: {best_logloss}\")\n",
    "print(f\"Best number of features: {best_num_features}\")\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_scaled.iloc[:, best_features]\n",
    "X_test_final = X_test_scaled.iloc[:, best_features]\n",
    "\n",
    "final_model = SVC(**best_params)\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred_final = final_model.predict(X_test_final)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "print(f\"Final test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shap bardzo dlugo zajmuje, więc zobacze na wstepie dla wszystkich cech jak sytuacja wygląda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-19 20:28:21,973] A new study created in memory with name: no-name-ef487da6-5f3e-4cb1-8811-27b30ecb2596\n",
      "[I 2024-12-19 20:28:42,463] Trial 0 finished with value: 0.651512376364475 and parameters: {'kernel': 'rbf', 'C': 3.907967156822881, 'gamma': 0.00042079886696066364}. Best is trial 0 with value: 0.651512376364475.\n",
      "[I 2024-12-19 20:29:35,941] Trial 1 finished with value: 0.5638345718239123 and parameters: {'kernel': 'poly', 'C': 4.0428727350273315, 'gamma': 0.06796578090758151, 'degree': 2}. Best is trial 0 with value: 0.651512376364475.\n",
      "[I 2024-12-19 20:29:48,515] Trial 2 finished with value: 0.6528281658381592 and parameters: {'kernel': 'linear', 'C': 0.012329623163659839}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:30:03,909] Trial 3 finished with value: 0.5009620253164556 and parameters: {'kernel': 'poly', 'C': 0.3905441275210791, 'gamma': 0.0014618962793704966, 'degree': 4}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:30:43,281] Trial 4 finished with value: 0.5662645928355456 and parameters: {'kernel': 'poly', 'C': 0.5450293694558254, 'gamma': 0.13826232179369857, 'degree': 2}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:31:04,507] Trial 5 finished with value: 0.6520182442474248 and parameters: {'kernel': 'rbf', 'C': 4.418441521199722, 'gamma': 0.00048094619675015767}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:31:18,674] Trial 6 finished with value: 0.5654565674165941 and parameters: {'kernel': 'poly', 'C': 70.85721663941601, 'gamma': 0.001653693718282443, 'degree': 2}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:31:55,144] Trial 7 finished with value: 0.6527266437759442 and parameters: {'kernel': 'linear', 'C': 0.9355380606452179}. Best is trial 2 with value: 0.6528281658381592.\n",
      "[I 2024-12-19 20:32:16,434] Trial 8 finished with value: 0.6534352482960079 and parameters: {'kernel': 'rbf', 'C': 9.443515687962675, 'gamma': 0.0017654048052495078}. Best is trial 8 with value: 0.6534352482960079.\n",
      "[I 2024-12-19 20:32:53,208] Trial 9 finished with value: 0.6005873007738431 and parameters: {'kernel': 'rbf', 'C': 656.9128640939175, 'gamma': 0.12604664585649453}. Best is trial 8 with value: 0.6534352482960079.\n"
     ]
    }
   ],
   "source": [
    "def wrapped_objective(trial):\n",
    "    return objective_svm_logloss(trial, X_train_scaled, y_train)\n",
    "study_svm = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.RandomSampler(seed=42))\n",
    "study_svm.optimize(wrapped_objective, n_trials=200, timeout=3600)\n",
    "best_params_svm = study_svm.best_params\n",
    "print(\"Best parameters:\", best_params_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_svm = SVC(**best_params_svm)\n",
    "final_model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = final_model_svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Final model accuracy on test data:\", accuracy_svm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
